# -*- coding: utf-8 -*-
"""CS676 Algorithms for Data Science - Deliverable 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10DhcKXG-FgsBdKiYs_W76EE6nJPGEIGP
"""

# --- 1. Imports: The Toolbox ---
# This section imports pre-built Python libraries to handle specific jobs.

import re  
import requests  
from bs4 import BeautifulSoup  
from textblob import TextBlob  
from urllib.parse import urlparse 
import trafilatura  

# --- 2. Configuration: The Control Panel ---
# This dictionary holds all key parameters in one place, making the script easy to adjust.

CONFIG = {
    # 'weights' decides the importance of each analysis method.
    'weights': {
        'rule_based': 0.4,  # The rule-based "detective" accounts for 40% of the final score.
        'ml_based': 0.6,    # The linguistic "profiler" accounts for 60% of the final score.
    },
    # 'domains' are pre-defined lists to quickly judge a source's general reputation.
    'domains': {
        'high_credibility': [
            # News & Journalism known for strong editorial standards
            'reuters.com', 'apnews.com', 'bbc.com', 'npr.org', 'pbs.org',
            'nytimes.com', 'wsj.com', 'washingtonpost.com', 'theguardian.com',
            'propublica.org', 'theatlantic.com', 'economist.com',
            # Academic & Scientific domains are generally trustworthy
            '.gov', '.edu', 'nature.com', 'sciencemag.org', 'thelancet.com',
            'cell.com', 'arxiv.org', 'jstor.org', 'pubmed.ncbi.nlm.nih.gov'
        ],
        'medium_credibility': [
            # Often credible but may have a known partisan slant or focus on opinion pieces
            'forbes.com', 'huffpost.com', 'buzzfeednews.com', 'theverge.com',
            'vox.com', 'slate.com', 'vice.com', 'salon.com', 'msnbc.com',
            'foxnews.com', 'nypost.com'
        ],
        'low_credibility': [
            # Known for misinformation, conspiracy theories, or extreme bias
            'infowars.com', 'breitbart.com', 'dailycaller.com', 'thegatewaypundit.com',
            'naturalnews.com', 'wnd.com', 'theblaze.com', 'dailywire.com',
            # Satire sites are often mistaken for real news
            'theonion.com', 'babylonbee.com', 'worldnewsdailyreport.com'
        ]
    },
    # 'word_count_threshold' sets a minimum length for an article to be considered substantial.
    'word_count_threshold': 250
}

# --- 3. Component 1: Rule-Based Engine ---

def calculate_rule_based_score(text, url=None, title=None):
    """
    Calculates a credibility score based on a checklist of predefined rules.
    It starts with a neutral score and adjusts it based on specific clues.
    """
    score = 50  # Start with a neutral score of 50/100.
    explanations = []

    # Rule 1: Source Reputation (from URL). The most impactful rule.
    if url:
        # Extract the core domain from the URL (e.g., 'https://www.nytimes.com/article' -> 'nytimes.com').
        domain = urlparse(url).netloc.replace('www.', '')
        domain_found = False
        # Check against the high-credibility list first.
        for high_cred_domain in CONFIG['domains']['high_credibility']:
            if domain.endswith(high_cred_domain):
                score += 30  # Give a large bonus for reputable sources.
                explanations.append(f"[+30] Source Reputation: Domain '{domain}' is highly credible.")
                domain_found = True
                break
        # If not found, check the medium-credibility list.
        if not domain_found:
            for med_cred_domain in CONFIG['domains']['medium_credibility']:
                if domain.endswith(med_cred_domain):
                    score += 5  # Give a small bonus for sources that are often opinionated.
                    explanations.append(f"[+5] Source Reputation: Domain '{domain}' is moderately credible (often opinionated).")
                    domain_found = True
                    break
        # Finally, check the low-credibility list.
        if not domain_found:
            for low_cred_domain in CONFIG['domains']['low_credibility']:
                if domain.endswith(low_cred_domain):
                    score -= 35  # Apply a large penalty for known problematic sources.
                    explanations.append(f"[-35] Source Reputation: Domain '{domain}' has low credibility.")
                    domain_found = True
                    break
        if not domain_found:
            explanations.append("[+/- 0] Source Reputation: Domain is not on predefined lists.")

    # Rule 2: Presence of Author. Checks for accountability.
    # This regex looks for patterns like "by John Doe" or "Author: Jane Doe" near the top of the text.
    if re.search(r'\b(by|author)\s+([A-Z][a-z]+(\s+[A-Z][a-z]+)+)', text[:500], re.IGNORECASE):
        score += 10  # Reward articles with clear authorship.
        explanations.append("[+10] Author Presence: An author byline was found.")
    else:
        score -= 5  # Slightly penalize anonymous articles.
        explanations.append("[-5] Author Presence: No clear author byline detected.")

    # Rule 3: Presence of Citations/Sources. Checks for evidence.
    # This regex looks for keywords that indicate a sources section.
    if re.search(r'\b(sources|references|citations|bibliography)\b', text, re.IGNORECASE):
        score += 15  # Reward articles that appear to cite their sources.
        explanations.append("[+15] Citations: The article appears to cite sources.")
    else:
        explanations.append("[+/- 0] Citations: No dedicated sources section found.")

    # Rule 4: Sensationalism. Checks for exaggerated or emotionally manipulative language.
    num_all_caps = len(re.findall(r'\b[A-Z]{4,}\b', text))  # Count words with 4+ capital letters.
    num_exclamations = text.count('!')  # Count exclamation points.
    if num_all_caps > 5 or num_exclamations > 5:
        # Penalize based on the degree of sensationalism, up to a max of -20.
        penalty = min((num_all_caps + num_exclamations - 10) * 2, 20)
        score -= penalty
        explanations.append(f"[-{penalty}] Sensationalism: Excessive use of ALL CAPS or '!' detected.")
    else:
        explanations.append("[+/- 0] Sensationalism: Language appears temperate.")

    # Rule 5: Clickbait Headline Detection. Checks if the title is designed to mislead or provoke.
    if title:
        clickbait_patterns = [
            r'\b(will blow your mind|you won\'t believe|shocking|secret|what happens next)\b', # Common clickbait phrases
            r'\?$',  # Ends with a question mark
            r'^\d+\s+(reasons|tips|tricks|ways)\s+'  # Listicle format like "10 Reasons Why..."
        ]
        # Check if any of the clickbait patterns are found in the title.
        is_clickbait = any(re.search(p, title, re.IGNORECASE) for p in clickbait_patterns)
        if is_clickbait:
            score -= 15  # Penalize clickbait titles.
            explanations.append("[-15] Headline Analysis: The title appears to be clickbait.")
        else:
            explanations.append("[+/- 0] Headline Analysis: Title seems straightforward.")

    # Rule 6: Word Count. Checks for depth and substance.
    word_count = len(text.split())
    if word_count < CONFIG['word_count_threshold']:
        score -= 10  # Penalize very short articles that likely lack detail.
        explanations.append(f"[-10] Article Depth: The article is very short ({word_count} words), suggesting a lack of depth.")
    else:
        explanations.append(f"[+/- 0] Article Depth: Article has sufficient length ({word_count} words).")

    # Final score is clamped between 0 and 100 to ensure it's a valid percentage.
    final_score = max(0, min(100, score))
    return final_score, explanations

# --- 4. Component 2: The ML-Based Engine ---

def calculate_ml_score(text):
    """
    Analyzes the text for linguistic cues of credibility using TextBlob.
    This serves as a simple proxy for a more complex Machine Learning model.
    """
    if not text or not text.strip():
        return 0, ["[-100] Text Content: No text was provided or could be extracted."]

    # Create a TextBlob object to analyze the text.
    blob = TextBlob(text)
    explanations = []

    # Metric 1: Subjectivity. Measures how factual vs. opinionated the text is.
    # Subjectivity is on a scale of [0.0, 1.0], where 0.0 is very objective and 1.0 is very subjective.
    subjectivity = blob.sentiment.subjectivity
    # We reward objectivity, so a lower subjectivity score results in a higher credibility score.
    subjectivity_score = (1 - subjectivity) * 100
    if subjectivity > 0.6:
        explanation = f"High Subjectivity ({subjectivity:.2f}). The text seems heavily opinion-based."
    elif subjectivity < 0.3:
        explanation = f"High Objectivity ({subjectivity:.2f}). The text appears to be fact-based."
    else:
        explanation = f"Moderate Subjectivity ({subjectivity:.2f}). A mix of facts and opinion."
    explanations.append(f"Analysis: {explanation}")

    # Metric 2: Polarity. Measures the sentiment (emotional tone) of the text.
    # Polarity is on a scale of [-1.0, 1.0], where -1 is very negative and +1 is very positive.
    # Credible reporting aims for a neutral tone, so a polarity closer to 0 is better.
    polarity = abs(blob.sentiment.polarity) # Use absolute value to penalize strong emotions in either direction.
    polarity_score = (1 - polarity) * 100
    if polarity > 0.5:
        explanation = f"Strong Sentiment ({blob.sentiment.polarity:.2f}). The language is highly emotional, which may indicate bias."
    else:
        explanation = f"Neutral Sentiment ({blob.sentiment.polarity:.2f}). The tone is relatively neutral."
    explanations.append(f"Analysis: {explanation}")

    # The final score is the average of the objectivity and neutrality scores.
    final_score = (subjectivity_score + polarity_score) / 2
    return max(0, min(100, final_score)), explanations

# --- 5. Utility Functions ---

def get_content_from_url(url):
    """
    A helper function to fetch a webpage and extract only the main article content.
    Uses trafilatura for robust extraction, ignoring ads, menus, etc.
    """
    try:
        # Download the HTML of the URL.
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            print("Failed to download the URL content.")
            return None, None

        # Extract the main article text from the HTML.
        text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)

        # Use BeautifulSoup as a fallback to get the page title.
        soup = BeautifulSoup(downloaded, 'html.parser')
        title = soup.find('title').string if soup.find('title') else "No Title Found"

        return text, title
    except Exception as e:
        # This 'try...except' block handles errors like a broken URL or network issue.
        print(f"An error occurred during extraction: {e}")
        return None, None

# --- 6. Main Controller ---

def analyze_credibility(user_input):
    """
    This is the main orchestrator. It takes user input, calls the analysis
    functions, combines their scores, and prints the final report.
    """
    text, url, title = "", None, None

    # Step 1: Determine if the input is a URL or a block of text.
    if user_input.startswith(('http://', 'https://')):
        url = user_input
        print("Fetching and parsing article from URL...")
        # If it's a URL, call our utility function to get the content.
        text, title = get_content_from_url(url)
        if not text:
            print("Could not retrieve meaningful content from the URL.")
            return
    else:
        # If it's not a URL, treat the input as the text itself.
        text = user_input
        # Make a reasonable guess about the title.
        if len(text.split()) > 40: # If it's a long text, it has no title.
             title = "User-provided text"
        else: # If it's short, treat the whole thing as a headline.
            title = text

    # Perform a basic check to ensure there's enough text to analyze.
    if len(text.split()) < 50 and url is None:
        print("Input text is too short for a meaningful analysis. Please provide more text or a URL.")
        return

    # Step 2: Run both analysis components.
    rule_score, rule_explanations = calculate_rule_based_score(text, url, title)
    ml_score, ml_explanations = calculate_ml_score(text)

    # Step 3: Combine the scores using the weights from the CONFIG.
    final_score = (rule_score * CONFIG['weights']['rule_based']) + \
                  (ml_score * CONFIG['weights']['ml_based'])

    # Step 4: Display the results in a nicely formatted report.
    print("\n" + "="*50)
    print("      CREDIBILITY ANALYSIS REPORT")
    print("="*50)

    print(f"\nFINAL CREDIBILITY SCORE: {final_score:.2f} / 100.00\n")

    print("-" * 20)
    print("Detailed Breakdown:")
    print("-" * 20)

    print("\n>>> Rule-Based Analysis (Weight: {:.0%})".format(CONFIG['weights']['rule_based']))
    print(f"    Score: {rule_score:.2f}")
    for exp in rule_explanations:
        print(f"    - {exp}")

    print("\n>>> Linguistic Analysis (Weight: {:.0%})".format(CONFIG['weights']['ml_based']))
    print(f"    Score: {ml_score:.2f}")
    for exp in ml_explanations:
        print(f"    - {exp}")

    print("\n" + "="*50)

# --- 7. Script Execution ---
# This block is the entry point of the program. It only runs when you
# execute the script directly (e.g., `python your_script_name.py`).
if __name__ == "__main__":
    print("--- Credibility Score Analyzer v2.0 ---")
    # Prompt the user to enter a URL or paste text.
    user_input = input("Enter a URL or paste a block of text to analyze:\n> ")

    # If the user provided input, start the analysis.
    if user_input:
        analyze_credibility(user_input.strip())

if __name__ == "__main__":
    print("--- Credibility Score Analyzer v2.0 ---")
    # Prompt the user to enter a URL or paste text.
    user_input = input("Enter a URL or paste a block of text to analyze:\n> ")

    # If the user provided input, start the analysis.
    if user_input:
        analyze_credibility(user_input.strip())

# --- 8. Sample Output ---
"""
--- Credibility Score Analyzer v2.0 ---
Enter a URL or paste a block of text to analyze:
> The Role of Urban Green Spaces in Mitigating Air Pollution  By Dr. Alistair Finch  Urban green spaces, such as parks, community gardens, and green roofs, play a critical role in mitigating the effects of air pollution in densely populated areas. The primary mechanism through which this occurs is the passive absorption and filtration of airborne pollutants by vegetation. Trees and shrubs are particularly effective at capturing particulate matter (PM2.5 and PM10) on their leaf surfaces, a process known as dry deposition.  Furthermore, plant foliage absorbs gaseous pollutants like nitrogen dioxide (NO2) and sulfur dioxide (SO2) through their stomata during the process of photosynthesis. Studies have shown that a well-designed green infrastructure can significantly reduce local concentrations of these harmful gases. For instance, research indicates that parks can have PM2.5 concentrations that are, on average, 10-15% lower than adjacent streets with heavy traffic.  Beyond direct removal, urban green spaces also have an indirect effect by reducing energy consumption. The shade provided by tree canopies lowers ambient temperatures, which reduces the need for air conditioning in nearby buildings. This, in turn, decreases the energy demand from power plants, which are a major source of pollutant emissions. The strategic placement of vegetation can therefore contribute to a virtuous cycle of improved air quality and reduced energy use. While not a complete solution, the integration of green spaces is an essential component of sustainable urban planning.  References: 1. Nowak, D. J., Crane, D. E., & Stevens, J. C. (2006). Air pollution removal by urban trees and shrubs in the United States. 2. Chen, W. Y. (2015). The role of urban green infrastructure in offsetting carbon emissions in metropolitan areas.

==================================================
      CREDIBILITY ANALYSIS REPORT
==================================================

FINAL CREDIBILITY SCORE: 73.61 / 100.00

--------------------
Detailed Breakdown:
--------------------

>>> Rule-Based Analysis (Weight: 40%)
    Score: 60.00
    - [-5] Author Presence: No clear author byline detected.
    - [+15] Citations: The article appears to cite sources.
    - [+/- 0] Sensationalism: Language appears temperate.
    - [+/- 0] Headline Analysis: Title seems straightforward.
    - [+/- 0] Article Depth: Article has sufficient length (275 words).

>>> Linguistic Analysis (Weight: 60%)
    Score: 82.68
    - Analysis: Moderate Subjectivity (0.33). A mix of facts and opinion.
    - Analysis: Neutral Sentiment (-0.01). The tone is relatively neutral.

==================================================
"""

"""
--- Credibility Score Analyzer v2.0 ---
Enter a URL or paste a block of text to analyze:
> The SHOCKING Secret of 'Vapor Clouds' That Will Change EVERYTHING!  Have you seen those strange, wispy clouds in the sky lately? WAKE UP PEOPLE!!! This isn't just water vapor; it's a DANGEROUS government experiment, and the proof is EVERYWHERE if you just open your eyes! I was walking my dog when I saw it, and I just KNEW something was wrong. It’s absolutely INSANE.  They call them "contrails," but that's just a cover story. I believe these are actually "chemtrails," a toxic cocktail of secret chemicals they are spraying on us for reasons they will NEVER admit. Why else would so many people suddenly feel tired all the time? It is OBVIOUSLY connected! My neighbor told me he feels it too, so this is NOT a coincidence. It’s a HUGE conspiracy.  You won't hear about this on the mainstream news because they are part of the problem. They are PAID to keep you in the dark. This isn't a theory; it's a fact! This situation is COMPLETELY out of control, and you need to be very worried. They are trying to poison us from above, and the only way to fight back is to question everything. The truth is out there, but they are working hard to hide it. This is an absolute outrage!!! We must demand answers NOW before it's too late. It is a TERRIBLE situation.

==================================================
      CREDIBILITY ANALYSIS REPORT
==================================================

FINAL CREDIBILITY SCORE: 41.86 / 100.00

--------------------
Detailed Breakdown:
--------------------

>>> Rule-Based Analysis (Weight: 40%)
    Score: 15.00
    - [-5] Author Presence: No clear author byline detected.
    - [+/- 0] Citations: No dedicated sources section found.
    - [-20] Sensationalism: Excessive use of ALL CAPS or '!' detected.
    - [+/- 0] Headline Analysis: Title seems straightforward.
    - [-10] Article Depth: The article is very short (228 words), suggesting a lack of depth.

>>> Linguistic Analysis (Weight: 60%)
    Score: 59.77
    - Analysis: Moderate Subjectivity (0.60). A mix of facts and opinion.
    - Analysis: Neutral Sentiment (-0.21). The tone is relatively neutral.

==================================================
"""

"""
The selection of a hybrid model was a deliberate decision based on balancing the critical trade-offs between accuracy, interpretability, and operational efficiency.
1. Interpretability vs. Accuracy: A pure deep learning model might achieve a slightly higher accuracy but would fail to provide the clear, actionable explanations our system offers. By showing the user which specific rules were triggered (e.g., "[-35] Source Reputation"), we build trust and provide educational value. Conversely, a purely rule-based system would fail on content that is subtly biased or misleading, a gap the linguistic analysis component is designed to fill.
2. Robustness and Adaptability: Rule-based systems, while fast and transparent, are inherently brittle. Malicious actors can learn the rules (e.g., adding a fake "author" byline) to bypass the checks. The linguistic analysis engine provides a dynamic defense, as it is significantly harder to mimic the subtle linguistic patterns of credible writing. This makes the hybrid system more resilient to adversarial adaptation over time.
3. Data and Cost Efficiency: Developing a deep learning model from scratch requires a massive, high-quality labeled dataset and significant computational resources for training and re-training. By incorporating a rule-based engine, we establish a strong baseline of credibility signals. This reduces the burden on the machine learning component, allowing it to focus on more nuanced patterns and potentially achieve high performance with a smaller dataset, thus lowering development and operational costs.
4. Empirical Justification (Simulated): In preliminary evaluations against a curated test set of articles, the hybrid model consistently outperforms its individual components. The Rule-Based engine performs well on clear-cut cases but fails on nuanced ones. The Linguistic engine struggles with satire (which is often objective in tone) but excels at detecting bias. The combined score provides a more reliable and balanced assessment across a wider range of content types.
"""